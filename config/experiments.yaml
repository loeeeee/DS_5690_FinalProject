# Placeholder experiment grid; fill with validated values after smoke tests.
hardware:
  gpu: nvidia_geforce_rtx_2080_ti  # default cluster target
  precision: bf16  # adjust if unsupported
  warmup_runs: 3

models:
  llama:
    name: llama-3-8b
    variant: fp16
  llada:
    name: llada-8b
    default_steps: 64
    step_sweep: [16, 32, 64, 128, 256]

prompts:
  source: wikitext-2  # replace with final dataset choice
  max_prompt_tokens: [128, 512]
  target_tokens: [128, 512, 1024]

benchmark:
  batch_sizes: [1, 4, 8, 16, 32]
  repeats: 3  # per config for averaging
  metrics:
    latency: [ttft, itl, wallclock]
    throughput: tokens_per_second
    memory: peak_vram
    quality: [perplexity, bertscore]

output:
  raw_dir: results/raw_data
  figures_dir: results/figures
  slurm_logs: logs/slurm

